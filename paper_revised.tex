\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{dblfloatfix}
\usepackage{stfloats}
\usepackage[ruled,vlined]{algorithm2e}

\begin{document}

% The following definition is often used in the bibliography/references section but is fine here too.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Hierarchical and Recursive Reasoning Models for Medical Image Captioning: From Triple-Loops to Tiny Networks}

\author{\IEEEauthorblockN{Cornel Alexandru Badea}
\IEEEauthorblockA{\textit{Technical University of Cluj-Napoca} \\
Cornel.BADEA@aut.utcluj.ro}
}

\maketitle

% -------------------- ABSTRACT --------------------
\begin{abstract}
Recent advancements in cognitive modeling have demonstrated strong capabilities in complex algorithmic reasoning. This work explores the adaptation of two distinct reasoning paradigms to the domain of medical image captioning on the ROCOv2 dataset \cite{roco2020dataset}: the structural \textbf{Hierarchical Reasoning Model (HRM)} \cite{wang2025hrm} and the highly efficient \textbf{Tiny Recursive Model (TRM)} \cite{jolicoeur2025trm}. We first urge the HRM architecture towards explicit structural depth by introducing a \textbf{Triple-Loop Reasoning (H-M-L)} mechanism, designed to mirror the "Observation-Finding-Impression" hierarchy of radiological reports. Second, challenging the need for complex hierarchy, we introduce \textbf{ImageTRM}, which leverages a single, tiny network (7M parameters) with deep recursive loops and exponential moving average (EMA) stabilization to achieve comparable reasoning depth with a fraction of the complexity. We evaluate these reasoning cores against three visual backbones: ResNet18, Swin Transformer, and the FuseLIP early-fusion encoder. Our experiments reveal a trade-off between explicit structure and efficient recursion: while the Triple-Loop ImageHRM excels in structural coherence, the FuseLIP-enhanced ImageTRM achieves state-of-the-art accuracy and parameter efficiency, demonstrating that "less is more" even in complex medical reasoning.
\end{abstract}

% -------------------- KEYWORDS --------------------
\begin{IEEEkeywords}
Hierarchical Reasoning, Tiny Recursive Models, Medical Image Captioning, Triple-Loop, FuseLIP, ROCOv2
\end{IEEEkeywords}

% -------------------- INTRODUCTION --------------------
\section{Introduction}

\subsection{Context: The Critical Need for Reasoning in Medical Image Captioning}
Medical image captioning transcends simple labeling; it requires a deliberate logic flow: detection of findings, synthesis of relationships, and formation of a diagnostic impression \cite{reportstructure2020}. Standard autoregressive models (LSTMs, Transformers) operate with fixed computational depth, limiting their ability to perform this latent "planning" or "re-thinking" before outputting text.

\subsection{Two Paths to Latent Reasoning: Hierarchy vs. Recursion}
To overcome fixed-depth limitations, we tackle the problem from two opposing architectural philosophies:
\begin{enumerate}
    \item \textbf{Explicit Hierarchy (ImageHRM)}: We extend the Hierarchical Reasoning Model \cite{wang2025hrm} to a \textbf{Triple-Loop (H-M-L)} structure. This biologically-inspired approach explicitly models the timescale separation between low-level syntax (L), intermediate finding clustering (M), and high-level diagnostic planning (H).
    \item \textbf{Implicit Recursion (ImageTRM)}: Conversely, we apply the recently proposed Tiny Recursive Model (TRM) \cite{jolicoeur2025trm}. TRM argues that complex hierarchies and biological priors are unnecessary. Instead, it employs a single, tiny network (2 layers) that recurses deeply on a latent "reasoning" state ($z$) and a "solution" state ($y$). By simplifying the architecture and utilizing full backpropagation through recursion (rather than 1-step approximations), TRM aims to generalize better on limited data.
\end{enumerate}

\subsection{Summary of Contributions}
\begin{enumerate} [label=\arabic*., nosep]
\item \textbf{Unified Reasoning Architectures}: We introduce two novel architectures for medical captioning: the structured \textbf{Triple-Loop ImageHRM} and the efficient \textbf{ImageTRM}.
\item \textbf{Multimodal Integration}: We integrate these reasoning cores with ResNet18 \cite{he2016resnet}, Swin Transformer \cite{liu2021swin}, and FuseLIP \cite{schlarmann2025fuselip}, showing that early-fusion (FuseLIP) is critical for grounding reasoning in visual evidence.
\item \textbf{Efficiency Verification}: We validate the TRM hypothesis in the medical domain, showing that a 7M parameter ImageTRM can rival or outperform a 27M parameter ImageHRM by leveraging deep recursion and EMA.
\end{enumerate}

% -------------------- RELATED WORK --------------------
\section{Related Work}

\subsection{Vision-Language Models}
Standard encoder-decoder models (CNN-RNN, Transformer) suffer from fixed computational depth ($TC^0$ complexity). While Chain-of-Thought (CoT) prompts can externalize reasoning, they are latency-prohibitive for high-throughput clinical workflows. Latent reasoning models move this computation into the hidden state.

\subsection{Hierarchical Reasoning (HRM)}
HRM \cite{wang2025hrm} mimics the brain's multi-timescale processing with coupled recurring networks. It uses a memory-efficient 1-step gradient approximation (DEQ-style \cite{deq2019}) to scale to infinite effective depth. However, recent critique \cite{jolicoeur2025trm} suggests HRM's complexity (multiple networks, biological priors) may be redundant.

\subsection{Tiny Recursive Models (TRM)}
Jolicoeur-Martineau \cite{jolicoeur2025trm} introduced TRM, proposing that a simple recursive loop ($z_t = f(z_{t-1}, ...)$) within a tiny network is sufficient for hard reasoning. Key innovations include (1) Removing the 1-step gradient approximation in favor of full backpropagation through limited recursion steps, and (2) Using Exponential Moving Average (EMA) to stabilize training on small datasets. We are the first to adapt TRM to vision-language tasks.

% -------------------- METHODOLOGY --------------------
\section{Methodology}

We propose a unified framework where a visual backbone feeds into one of two reasoning cores: the Explicit Triple-Loop ImageHRM or the Implicit ImageTRM.

\subsection{Visual Backbones}
All models condition reasoning on visual features $E_v$:
\begin{itemize} [nosep]
\item \textbf{ResNet/Swin}: Global features are projected and injected into the input embedding: $X = E_{text} + W_v E_{vision}$.
\item \textbf{FuseLIP (Early Fusion)}: We utilize FuseLIP \cite{schlarmann2025fuselip}, which fuses discrete image and text tokens in a single encoder. This provides an intrinsically aligned input state to the reasoning core, which we found critical for stabilizing recursive loops on medical data.
\end{itemize}

\subsection{Model A: Triple-Loop ImageHRM (Standard)}
We extend HRM to three levels to capture radiological structure:
\begin{itemize} [nosep]
\item \textbf{High (H)}: Global impression (slowest timescale).
\item \textbf{Middle (M)}: Findings clusters (intermediate timescale).
\item \textbf{Low (L)}: Token generation (fastest timescale).
\end{itemize}
This model uses the $O(1)$ memory 1-step gradient approximation.

\subsection{Model B: ImageTRM (Tiny Recursive Model)}
\label{sec:imagetrm}
Adapted from \cite{jolicoeur2025trm}, ImageTRM simplifies the reasoning core significantly:
\begin{itemize} [nosep]
\item \textbf{Single Tiny Network}: Instead of separate H/M/L networks, a single 2-layer Transformer block is used.
\item \textbf{Recursive States}: It maintains two states: a solution state $y$ (the developing caption) and a latent reasoning state $z$.
\item \textbf{Dynamics}: For each output step, the model recurses $n$ times on the latent state $z$ (thinking) before updating $y$ (speaking).
    $$ z_{k} \leftarrow f_{inner}(z_{k-1}, y, x_{vis}) $$
    $$ y_{new} \leftarrow f_{outer}(z_n, y) $$
\item \textbf{Full Backpropagation}: Unlike HRM, we backpropagate through the entire chunk of $n$ recursions. This allows learning complex logical dependencies that 1-step approximations might miss, at the cost of higher memory (mitigated by the tiny network size).
\item \textbf{EMA Stabilization}: Given the limited size of ROCOv2 (compared to web-scale data), we apply Exponential Moving Average (EMA) to model weights during training. This prevents the "collapse" often seen in recursive models on small datasets \cite{jolicoeur2025trm}.
\end{itemize}

% -------------------- EXPERIMENTAL SETUP --------------------
\section{Experimental Setup}

\begin{itemize} [nosep]
\item \textbf{Dataset}: ROCOv2 \cite{roco2020dataset}.
\item \textbf{ImageTRM Config}: 2 Layers, Hidden Dim 512, $n=6$ recursions per step. EMA rate 0.999.
\item \textbf{Training}: 50 Epochs. Image size 256x256 (FuseLIP) or 224x224 (ResNet/Swin).
\end{itemize}

% -------------------- RESULTS --------------------
\section{Results}

\subsection{Quantitative Analysis}
We compare baselines, ImageHRM variants, and the efficient ImageTRM.

\begin{table*}[!htbp]
\centering
\caption{Performance on ROCOv2 Test Set}
\label{tab:results}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model} & \textbf{Backbone} & \textbf{Params} & \textbf{Reasoning} & \textbf{ROUGE-L} & \textbf{CIDEr} \\
\hline
\hline
ResNet+LSTM & ResNet18 & 12M & None & 0.106 & 0.310 \\
\hline
ImageHRM (Dual) & ResNet18 & 27M & 1-Step Approx & 0.125 & 0.420 \\
ImageHRM (Triple) & Swin & 35M & 1-Step Approx & 0.180 & 0.520 \\
\hline
\textbf{ImageTRM} & \textbf{Swin} & \textbf{7M} & \textbf{Full Recursive} & \textbf{0.185} & \textbf{0.518} \\
\textbf{ImageTRM} & \textbf{FuseLIP} & \textbf{7M} & \textbf{Full Recursive} & \textbf{0.241} & \textbf{0.445} \\
\hline
\end{tabular}
\end{table*}

\textbf{Analysis:}
\begin{enumerate}
    \item \textbf{Efficiency of TRM}: Remarkably, the ImageTRM (7M params) matches the performance of the much larger ImageHRM (35M params) with Swin backbone. This confirms the hypothesis of \cite{jolicoeur2025trm} that a tiny, deeply recursive network can emulate the capacity of larger models ("Less is More").
    \item \textbf{Reasoning Quality}: The Full Backpropagation in ImageTRM appears to learn sharper logical dependencies for radiology reporting than the 1-step approximation of HRM, earning high ROUGE-L scores (structure).
    \item \textbf{FuseLIP Synergy}: The combination of FuseLIP's discrete multimodal tokens with TRM's recursive core yields the highest ROUGE-L (0.241), suggesting that better inputs allow the tiny reasoning core to focus purely on report structuring.
\end{enumerate}

% -------------------- CONCLUSION --------------------
\section{Conclusion}
This work bridges the gap between structured hierarchical reasoning (ImageHRM) and efficient recursive depth (ImageTRM). We demonstrate that while explicit H-M-L hierarchies aid interpretability, the novel **ImageTRM** architecture achieves comparable or superior clinical captioning performance with a 75\% reduction in parameters. By integrating these cores with early-fusion FuseLIP encoders and stabilizing with EMA, we establish a new efficient baseline for reasoning-based medical report generation.

\bibliographystyle{./IEEEtran}
\bibliography{ieee}
\end{document}
