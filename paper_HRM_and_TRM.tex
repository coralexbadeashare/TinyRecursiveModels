\documentclass[conference]{IEEEtran}
\IEEEoverridecommandlockouts
% The preceding line is only needed to identify funding in the first footnote. If that is unneeded, please comment it out.
\usepackage{cite}
\usepackage{amsmath,amssymb,amsfonts}
\usepackage{graphicx}
\usepackage{enumitem}
\usepackage{textcomp}
\usepackage{xcolor}
\usepackage{dblfloatfix}
\usepackage{stfloats}
\usepackage[ruled,vlined]{algorithm2e}

\begin{document}

% The following definition is often used in the bibliography/references section but is fine here too.
\def\BibTeX{{\rm B\kern-.05em{\sc i\kern-.025em b}\kern-.08em T\kern-.1667em\lower.7ex\hbox{E}\kern-.125emX}}

\title{Hierarchical and Tiny Recursive Models for Medical Image Captioning}

\author{\IEEEauthorblockN{Cornel Alexandru Badea}
\IEEEauthorblockA{\textit{Technical University of Cluj-Napoca} \\
Cornel.BADEA@aut.utcluj.ro}
}

\maketitle

% -------------------- ABSTRACT --------------------
\begin{abstract}
Hierarchical and recursive reasoning are emerging as critical capabilities for overcoming the fixed-depth limitations of standard transformer architectures, particularly in domains requiring structured logical synthesis like medical image captioning. While recent Hierarchical Reasoning Models (HRM) have shown promise by mimicking multi-timescale cognitive processes \cite{wang2025hrm}, their application to vision-language tasks remains underexplored. In this work, we introduce a unified \textbf{ImageHRM} framework that integrates high-performance visual backbones (ResNet, Swin Transformer, and the multimodal FuseLIP) into a recurrent reasoning core. We propose a novel \textbf{Triple-Loop (H-M-L)} architecture designed to mirror the radiological workflow, utilizing intermediate semantic clustering to bridge low-level observations and high-level diagnostic impressions. Furthermore, we investigate the "Less is More" paradigm through the \textbf{Tiny Recursive Model (TRM)} \cite{trm2025}, which radically simplifies the architecture to a single, shared-weight network (only $\sim$7M parameters) trained via full backpropagation through time (BPTT). Evaluating these systems on the large-scale ROCOv2 dataset, we provide a rigorous comparative analysis of reasoning depth versus visual feature quality. Our results demonstrate that while the Triple-Loop ImageHRM with FuseLIP establishes a strong baseline, the computationally efficient \textbf{ImageTRM-Swin} variant achieves a new state-of-the-art. This surprising finding suggests that deep recursive reasoning, when seeded with robust hierarchical visual features, can outperform significantly larger and more complex systems, offering a scalable path toward high-fidelity automated radiology reporting.
\end{abstract}

% -------------------- KEYWORDS --------------------
\begin{IEEEkeywords}
Hierarchical Reasoning Models, Medical Image Captioning, Triple-Loop Reasoning, FuseLIP, Multimodal Encoders, ROCOv2
\end{IEEEkeywords}

% -------------------- INTRODUCTION --------------------
\section{Introduction}

\subsection{Context: The Critical Need for Reasoning in Medical Image Captioning}
Medical image captioning represents a complex vision-language task that transcends simple descriptive labeling. The accurate generation of a radiological report requires logical steps that extend far beyond merely detecting anatomical structures or pathological findings within an image. A clinically useful report must not only identify objects but also establish hierarchical relationships between multiple observations, synthesize these relationships into an overarching interpretation (the \textit{Impression}), and structure the output text adhering to professional standards (e.g., listing detailed findings before presenting a final impression) \cite{reportstructure2020}.

The requirement to translate multi-modal information into structured, coherent diagnostic text necessitates a deliberate, multi-step logical process, analogous to human radiological analysis. For instance, moving from the observation of specific, low-level \textit{Findings} (e.g., a small calcification, interstitial thickening) to a high-level \textit{Impression} (e.g., No evidence of acute obstruction) requires complex, internal algorithmic search and refinement. The robustness and clinical utility of the generated caption depend directly on the model's capacity for this advanced, latent computation.

\subsection{Problem: Limitations of Fixed-Depth Language Models in Generating Structured Reports}
Standard autoregressive models, such as LSTMs or traditional Transformer architectures \cite{vaswani2017attention}, inherently struggle with tasks requiring deep algorithmic planning. These models possess a fixed computational depth, which places them into computational complexity classes, such as $AC^0$ or $TC^0$, preventing them from executing the complex, polynomial-time algorithms necessary for deliberate, multi-stage reasoning.

This fixed-depth limitation often leads to critical errors in the medical context, such as generating superficially fluent but logically shallow captions or violating the explicit hierarchical structure of the radiological report (e.g., mixing findings with the final impression). While Chain-of-Thought (CoT) prompting has been widely adopted in general language models to mitigate this limitation by externalizing internal computation into sequential text, CoT is often brittle, relies on human-defined decompositions, and requires excessive data and high latency, which is sub-optimal for high-throughput clinical reporting. A more robust, efficient method of executing computation in the model's internal hidden state space---known as latent reasoning---is required to sustain lengthy, coherent chains of thought without externalizing them as language.

\subsection{Solution: Adapting Hierarchical Reasoning for Deliberate Planning}
To overcome the fixed-depth limitation and enable efficient latent reasoning, the Hierarchical Reasoning Model (HRM) is adopted and adapted \cite{wang2025hrm}. HRM is a recurrent architecture inspired by the hierarchical and multi-timescale processing observed in the human brain, where computation is organized across cortical regions operating at different speeds. This structure, featuring coupled recurrent modules, significantly increases the model's effective computational depth (potentially $N \times T$ steps).

HRM provides a mechanism for robust latent reasoning, allowing the model to perform extensive internal computation---including planning, search, and iterative refinement---before generating the next output token. Furthermore, HRM employs a memory-efficient one-step gradient approximation, resulting in an $O(1)$ memory complexity independent of sequence length, which is crucial for handling long radiological reports where standard Backpropagation Through Time (BPTT) would require prohibitive $O(T)$ memory \cite{deq2019}.

\subsection{Summary of Contributions}
The primary contributions of this work are summarized as follows:
\begin{enumerate} [label=\arabic*., nosep]
\item \textbf{ImageHRM Integration}: The introduction of a novel unified architecture that successfully integrates high-performance visual backbones (ResNet18 \cite{he2016resnet}, Swin Transformer \cite{liu2021swin}, and FuseLIP \cite{schlarmann2025fuselip}) with the recurrent, reasoning core of the HRM.
\item \textbf{Triple-Loop Architecture (H-M-L)}: The extension of the standard Dual-Loop HRM to a three-tiered structure. The inclusion of a Middle (M) layer explicitly models the required semantic clustering necessary to bridge abstract planning (H) and token execution (L), optimizing the generation of complex, structured medical reports.
\item \textbf{FuseLIP Multimodal Injection}: A variant utilizing the early fusion of discrete tokens via the FuseLIP encoder, providing the reasoning core with a uniquely pre-aligned multimodal embedding.
\item \textbf{Evolution to Tiny Recursive Models (TRM)}: We further introduce \textbf{ImageTRM}, a paradigm shift based on the "Less is More" principle \cite{trm2025}. We detail the training of three ImageTRM variants (ResNet, Swin, FuseLIP) which utilize a single "tiny" recurrent network to achieve superior generalization.
\item \textbf{Evaluation}: A rigorous comparative analysis of ImageHRM variants (Dual vs. Triple Loop, and varying backbones) on the challenging, clinical ROCOv2 radiology dataset \cite{roco2020dataset}.
\end{enumerate}

% -------------------- RELATED WORK --------------------
\section{Related Work}

\subsection{Vision-Language Models for General Image Captioning}
The cornerstone of modern image captioning is the Encoder-Decoder paradigm, translating visual features into textual sequences. Early and foundational approaches, often termed CNN-RNN models, utilized Convolutional Neural Networks (CNNs), such as ResNet \cite{he2016resnet}, as the visual encoder, and Recurrent Neural Networks (RNNs), such as LSTMs or GRUs, as the sequence decoder. These systems primarily employ two integration strategies: the "Inject" architecture, where the image feature vector initializes the decoder's hidden state, and the "Merge" or "Multi-Modal" architecture, where visual features are continuously combined with text embeddings throughout the decoding process, which has been shown to produce better results by maintaining continuous visual grounding.

The development of the Transformer architecture \cite{vaswani2017attention} further pushed performance by replacing RNNs with the self-attention mechanism, which is superior at handling long-range dependencies and is more parallelizable. However, the intrinsic limitation remains their fixed, non-recurrent depth, which restricts their capacity for deep algorithmic reasoning, a factor particularly problematic for structured text generation. Other variants like Dense Captioning \cite{johnson2016densecap} attempted to address scene complexity by generating multiple captions for different image regions, a concept with parallels to generating clustered findings in radiology reports. The efficacy of the ResNet+LSTM structure as a strong, but fixed-depth, baseline remains important for evaluating the benefits of newer, more complex architectures like HRM.

\subsection{Hierarchical Reasoning and Latent Computation}
The necessity to overcome the depth constraints of $AC^0/TC^0$ models led to the exploration of architectures capable of algorithm learning and universal computation \cite{wang2025hrm}. Early efforts included Neural Turing Machines (NTM) and Universal Transformers \cite{dehghani2018ut}, which introduced recurrence to increase the computational depth of computation.

The Hierarchical Reasoning Model (HRM) directly addresses the fixed-depth limitation by drawing inspiration from the brain's multi-timescale processing, where different cortical regions operate at distinct rhythms (e.g., slow theta waves vs. fast gamma waves). The core of HRM features two coupled recurrent modules---a high-level ($z_H$) for slow, abstract planning and a low-level ($z_L$) for fast, detailed computations---which iteratively refine internal representations. This structure achieves hierarchical convergence, preventing the premature stall of computation that often plagues standard RNNs, thereby providing an enhanced effective depth of $N \times T$ steps \cite{wang2025hrm}. This allows HRM to tackle tasks demanding extensive search and backtracking, such as Sudoku-Extreme and Maze-Hard, where fixed-depth models fail completely.

Crucially, HRM addresses the scalability issue of recurrent models by implementing a one-step gradient approximation (similar to techniques used in Deep Equilibrium Models, DEQ \cite{deq2019}) during training. This eliminates the need for Backpropagation Through Time (BPTT), which has an $O(T)$ memory footprint, by maintaining a constant memory footprint of \textbf{$O(1)$} regardless of the sequence length $T$. This technical feature is vital for applying deep recurrent reasoning to domains like radiology, where output sequences (reports) are often long and variable. Furthermore, HRM integrates Adaptive Computational Time (ACT) \cite{act2016}, which allows the model to dynamically allocate more computational resources (longer reasoning traces) only to more complex inputs, optimizing inference speed for routine cases.

\subsection{Multimodal Embedding via Early Fusion (FuseLIP)}
Most modern Vision-Language Models (VLMs), such as CLIP, rely on late fusion, where distinct encoders process image and text separately, and representations are aligned only in a final latent space via contrastive learning. This restricts modality interaction to high-level features.

FuseLIP \cite{schlarmann2025fuselip} introduces a novel early fusion approach based on discrete tokenization. It utilizes discrete image tokenizers (like TiTok) to map both the input image and text into a unified sequence of discrete tokens. This concatenated sequence is then processed by a *single* Transformer encoder. This single-encoder design allows the modalities to interact at every depth of encoding, leading to richer, fully merged representations that capture fine-grained image-text relationships. FuseLIP is trained using a combination of the SigLIP contrastive loss and a Masked Multimodal Modeling (MMM) loss. Because of the discrete tokenization, the MMM loss can be seamlessly incorporated without requiring auxiliary modules or additional computational overhead, unlike previous multimodal modeling attempts. This strategy has been shown to yield superior performance in challenging multimodal tasks that depend heavily on joint visual-textual structure.

\subsection{Domain-Specific Vision-Language Pretraining and Backbones}
Applying general VLM techniques to medical imaging is difficult due to the limited annotated datasets, unintuitive image contrasts, and nuanced visual features found in clinical data. Therefore, domain-specific pretraining is essential.

Models such as PubMedCLIP fine-tune the CLIP architecture on the ROCO dataset, while MedCLIP \cite{wang2021medclip} utilizes advanced backbones like BioClinicalBERT and the Swin Transformer \cite{liu2021swin}, pre-trained on datasets like MIMIC-CXR \cite{mimiccxr2019}. The Swin Transformer \cite{liu2021swin} is particularly effective as a visual encoder in the medical domain due to its hierarchical structure and shifted window attention mechanism, which allows it to efficiently capture multi-scale pathological features.

\subsection{Tiny Recursive Networks}
While HRM introduced the power of recursive reasoning, recent work on \textbf{Tiny Recursive Models (TRM)} \cite{trm2025} questions the necessity of its complex biological constraints. TRM demonstrates that a single, extremely small network (e.g., 2 layers) can outperform larger models by simply increasing the recursion depth ($N \times T$). Unlike HRM, which relies on a memory-efficient but potentially unstable 1-step gradient approximation, TRM's tiny size allows for \textbf{Full Backpropagation Through Time (BPTT)}, ensuring precise gradient calculation across the entire reasoning chain. This "Less is More" approach has shown remarkable generalization on algorithmic tasks (Sudoku, Maze), suggesting that the depth of the reasoning path is more critical than the width or complexity of the network layers.

% -------------------- METHODOLOGY --------------------
\section{Methodology}

\subsection{Network Architecture}
We propose ImageHRM, a model that conditions hierarchical reasoning on visual inputs.

\begin{figure*}[h!]
\centering
\includegraphics[width=0.7\textwidth]{ImageHRM.png}
\caption{ImageHRM: Unified Model with Vision Module}
\label{fig:unified_model}
\end{figure*}

\textit{Description}: The figure should show the visual backbone (ResNet18, Swin Transformer, or FuseLIP) processing an input radiological image $I$. For the non-fused backbones (ResNet \cite{he2016resnet}, Swin \cite{liu2021swin}), the resulting visual feature vector $V$ is projected to the HRM's hidden dimension $d_{model}$ and then element-wise added to the embedding of every text token $E_t$. This combined representation $X_t = E_t + E_v$ feeds into the recurrent reasoning loops. For the FuseLIP variant \cite{schlarmann2025fuselip}, the image and text are first tokenized into a unified discrete sequence, processed by the single FuseLIP transformer encoder, and the resulting integrated embedding is injected into the HRM recurrent core. The diagram should illustrate the input flow to the nested recurrent modules $f_L, f_M, f_H$.

\subsection{Visual-Textual Integration}
To ground the reasoning process in visual evidence, we utilize a ResNet18 backbone \cite{he2016resnet} initialized with ImageNet weights. 

\begin{itemize} [nosep]
\item \textbf{Feature Extraction and Projection}: The classification head is removed, and the global average pooled features are extracted. A linear layer projects visual features to the HRM embedding space: $E_v = W_v V$.
\item \textbf{Injection}: The visual embedding $E_v$ is added to the token embedding $E_t$ at each step: $Input = E_t + E_v$. This Merge architecture ensures the "mind" of the model always has access to the visual context.
\item \textbf{Swin Transformer Integration}: The Swin Transformer Base model \cite{liu2021swin} is utilized for its hierarchical structure and ability to capture multi-scale visual features. Global pooled features from the Swin encoder are projected and injected identically to the ResNet strategy.
\item \textbf{FuseLIP Early Fusion Integration}: For the FuseLIP variant \cite{schlarmann2025fuselip}, the single, pre-trained FuseLIP encoder produces an intrinsically integrated multimodal feature vector through early fusion of discrete image and text tokens. This integrated embedding replaces the standard token embedding $E_t$ as input to the recurrent modules, allowing the H-M-L core to focus solely on algorithmic sequencing and planning.
\end{itemize}

\subsection{Triple-Loop Hierarchical Reasoning (H-M-L)}
We extend the standard Dual-Loop (H-L) reasoning of the original HRM to a Triple-Loop structure to handle the complexity of medical text, explicitly modeling the cognitive process of a radiologist (Observation $\to$ Finding Cluster $\to$ Impression) \cite{reportstructure2020}.
\begin{figure}[h]
\centering
\fbox{\includegraphics[width=0.15\textwidth]{HRM_blocks.png}}
\caption{Diagram of Triple-Loop HRM Module}
\end{figure}
\textit{Description}: A schematic showing three nested loops:
\begin{itemize} [nosep]
\item \textbf{Outer Loop (H)}: High-level abstract planning (e.g., global diagnosis).
\item \textbf{Middle Loop (M)}: Intermediate semantic clustering (e.g., anatomical regions, specific findings).
\item \textbf{Inner Loop (L)}: Low-level syntax and token generation.
\end{itemize}
The diagram should illustrate the top-down ($H \to M \to L$) and bottom-up ($L \to M \to H$) information flow and the timescale separation leading to hierarchical convergence.

\textbf{Dynamics:}
\begin{algorithm}
\caption{Dynamics of Hierarchical Reasoning}
\For{each H-cycle}{
    \For{each M-cycle}{
        \For{each L-cycle}{
             Update $z_L$ based on $z_L$ and Input
        }
        Update $z_M$ based on $z_M$ and $z_L$
    }
    Update $z_H$ based on $z_H$ and $z_M$
}
\end{algorithm}


This architecture allows the model to "think" at three timescales simultaneously, with the $z_M$ state enforcing structural coherence by aggregating local $z_L$ execution before updating the long-term $z_H$ planning state.

\subsection{FuseLIP Pipeline Stabilization and Comparative Pre-training Analysis}
The successful integration of the FuseLIP multimodal encoder \cite{schlarmann2025fuselip}, which utilizes an early fusion mechanism based on synchronized discrete tokenization, required extensive pipeline stabilization. This process was critical for ensuring that the unified token sequence, incorporating both image and text inputs, was consistently presented to the single Transformer encoder.

\begin{itemize} [nosep]
\item \textbf{Tokenization Mechanism Verification}: A primary technical task involved the repair and rigorous verification of the discrete tokenization pipeline (including the image tokenizer and the text tokenizer). This was essential to ensure tokens were correctly concatenated and masked for the Masked Multimodal Modeling (MMM) loss during training and for accurate feature extraction during inference.
\item \textbf{Comparative Pre-training Evaluation}: To quantify the benefit of domain-specific adaptation, a comprehensive evaluation script was developed to compare the performance of a standard CC3M-pre-trained FuseLIP model against the same model fine-tuned on the medical ROCOv2 dataset \cite{roco2020dataset}. This script involved executing a complete forward pass on the standardized ROCOv2 test set and computing raw image-text cosine similarities, ensuring all features were properly normalized.
\item \textbf{Impact of Fine-Tuning on Alignment}: This comparative analysis demonstrated a significant and necessary increase in the model's ability to ground specific medical features after fine-tuning on ROCOv2. The alignment confidence (cosine similarity) for medical captions increased dramatically. For instance, for a sample test image (ROCOv2\_2023\_test\_000036.jpg), the similarity score for the correct medical caption ("Operative planning ultrasound...") increased from an ungrounded baseline of $-0.0253$ (CC3M pre-train) to a highly confident $0.4560$ post-ROCOv2 fine-tuning. A similar trend was observed for other examples (e.g., $0.3026$ to $0.4362$ for image 000002). This superior initial multimodal alignment—achieved through the pipeline stabilization and fine-tuning—relieves the ImageHRM recurrent core of basic modality alignment tasks, allowing it to focus entirely on the complex algorithmic sequencing and planning of the report structure.
\end{itemize}

\subsection{Further Analysis of FuseLIP Fine-Tuning Impact}
The quantitative jump in image-text alignment confidence validates the necessity of domain-specific fine-tuning for the early-fusion backbone. The magnitude of improvement (in some cases over a $1900\%$ increase in confidence for the correct medical caption) directly contributes to the superior performance of the final ImageHRM (Triple) + FuseLIP model observed in Section V, as the reasoning loops are provided with an intrinsically more integrated and medically relevant latent representation.
\par
All model training and fine-tuning experiments were conducted on a single NVIDIA L40S GPU. The Image HRM model \cite{wang2025hrm} was trained for a total duration of 6 hours over 50 epochs. The Fuselip model's \cite{schlarmann2025fuselip} training consisted of two phases: initial pretraining on the CC3M dataset for 8 epochs, followed by domain-specific fine-tuning on the medical dataset ROCOV2 \cite{roco2020dataset} for 20 epochs. This detailed setup ensures the reproducibility of our reported results.
% -------------------- TINY RECURSIVE MODELS --------------------
\section{Evolution to Tiny Recursive Models (TRM)}
\label{sec:trm}
While ImageHRM demonstrates the power of hierarchical reasoning, its reliance on multiple networks and complex biological justifications introduces structural redundancy. Addressing this, we present \textbf{ImageTRM}, an evolution based on the "Less is More" philosophy \cite{trm2025}.

\subsection{The "Less is More" Paradigm}
The Tiny Recursive Model (TRM) challenges the assumption that parameter count equates to reasoning capability. Unlike ImageHRM, which separates planning (H) and execution (L) into distinct neural networks ($\sim$27M parameters), ImageTRM utilizes a \textbf{single, tiny network} (only 2 layers, $\sim$7M parameters) to perform both functions through deep recursion. This massive reduction in parameters ($<1\%$ of typical LLMs) prevents overfitting on smaller medical datasets while maintaining the capacity for complex logical inference through extended recursive depth ($N \times T$).

\begin{figure}[h]
\centering
\includegraphics[width=\columnwidth]{ImageTRM.png}
\caption{ImageTRM Architecture: The vision module injects features directly into the recurrent loop of the single tiny core.}
\label{fig:trm_arch}
\end{figure}

\subsection{ImageTRM Architecture}
ImageTRM employs a recursive state update mechanism: $z_{t+1}, y_{t+1} = f_{net}(z_t, y_t, x)$.
\begin{itemize}
    \item \textbf{Single Shared Core}: A single transformer block processes the latent state $z$ and output state $y$ repeatedly. This shared-weight approach forces the model to learn universal reasoning operators rather than layer-specific heuristics.
    \item \textbf{Full Backpropagation}: By leveraging the tiny footprint of the core network, ImageTRM avoids the need for the error-prone "one-step gradient approximation" used in HRM. Instead, we perform full Backpropagation Through Time (BPTT) through the entire recursive chain, ensuring precise gradient flow and robust convergence.
    \item \textbf{EMA Stabilization}: To mitigate the instability inherent in deep recursive loops, we employ Exponential Moving Average (EMA) on the weights, which smoothens the optimization landscape and is critical for the convergence of tiny recursive networks.
\end{itemize}

\subsection{System Training and Verification}
We have successfully implemented and trained three variants of the ImageTRM system on the ROCOv2 dataset, creating a comprehensive suite of efficient reasoning models:
\begin{enumerate}
    \item \textbf{ImageTRM-ResNet}: Integrates the robust ResNet18 visual backbone. This model serves as the efficient baseline, proving that deep recursive reasoning can be driven by standard convolutional features.
    \item \textbf{ImageTRM-Swin}: Incorporates the Swin Transformer backbone. This variant leverages multi-scale attention to feed a richer, hierarchical visual context into the tiny recursive reasoning core.
    \item \textbf{ImageTRM-FuseLIP}: The most advanced variant, utilizing the FuseLIP early-fusion backbone. By feeding a sequence of discrete, pre-aligned image-text tokens into the TRM core, this system maximizes the density of information available for recursive logical planning.
\end{enumerate}
All three systems have been trained for 50 epochs, demonstrating stable loss convergence and confirming the viability of the "Tiny Recursive" paradigm for medical image captioning.

% -------------------- EXPERIMENTAL SETUP --------------------
\section{Experimental Setup}

\begin{itemize} [nosep]
\item \textbf{Dataset}: ROCOv2 (Radiobiology Images and Captions) \cite{roco2020dataset}.
\item Size: 79,789 images in total, including 59,958 images in the training set and 9,927 images in the test set.
\item Preprocessing: Resize to 224x224, Tokenization (ASCII/BPE).
\item \textbf{Implementation Details}:
\item \textbf{Backbone}: ResNet18 (Frozen) \cite{he2016resnet}, Swin-Base (Frozen) \cite{liu2021swin}, FuseLIP (Fine-Tuned) \cite{schlarmann2025fuselip}.
\item \textbf{Reasoning Depth}: H=2 layers, M=2 layers, L=2 layers for the blocks. The architecture is trained using the \textbf{$O(1)$ memory-efficient one-step gradient approximation} method \cite{deq2019}.
\item \textbf{Sequence Length}: 512 tokens.
\item \textbf{Training}: 50 Epochs, AdamW optimizer \cite{loshchilov2019} with Adaptive Computation Time (ACT) enabled \cite{act2016}.
\item \textbf{Evaluation Metrics}: Standard image captioning metrics that emphasize both textual fluency and clinical relevance ROUGE-L (for structural flow) \cite{rouge2004}, and CIDEr (for consensus) \cite{cider2015}.
\end{itemize}

% -------------------- RESULTS --------------------
\section{Results}

\subsection{Quantitative Analysis}
We compare the standard Dual-Loop HRM against our proposed Triple-Loop configurations with various backbones.

\begin{table*}[!htbp]
\centering
\caption{Quantitative Results on ROCOv2}
\label{tab:results}
\begin{tabular}{|l|c|c|c|c|c|c|c|}
\hline
\textbf{Model Variant} & \textbf{Backbone} & \textbf{H/M/L Config} & \textbf{ACT Loss} & \textbf{ROUGE-L} & \textbf{CIDEr} \\
\hline
\hline
ResNet+LSTM (Baseline) & ResNet18 \cite{he2016resnet} & N/A & 1.87 & 0.106 & 0.310 \\
\hline
ImageHRM (Dual) & ResNet18 \cite{he2016resnet} & 1/0/1 & 0.53 & 0.125 & 0.420 \\
\hline
ImageHRM (Triple) & ResNet18 \cite{he2016resnet} & 1/1/1 & 0.49 & 0.157 & 0.478 \\
\hline
ImageHRM (Triple) & Swin \cite{liu2021swin} & 1/1/1 & 0.45 & 0.180 & 0.52 \\
\hline
ImageHRM (Triple) & FuseLIP \cite{schlarmann2025fuselip} & 1/1/1 & \textbf{0.40} & \textbf{0.234} & \textbf{0.438} \\
\hline
\end{tabular}
\end{table*}

\begin{table*}[!htbp]
\centering
\caption{Quantitative Results on ROCOv2 (TRM Retraining)}
\label{tab:results_trm}
\begin{tabular}{|l|c|c|c|c|c|c|}
\hline
\textbf{Model Variant} & \textbf{Paradigm} & \textbf{Backbone} & \textbf{ROUGE-L} & \textbf{CIDEr} & \textbf{Notes} \\
\hline
\hline
ImageHRM (Triple) & Hierarchical & FuseLIP \cite{schlarmann2025fuselip} & \textbf{0.234} & 0.438 & Previous SOTA \\
\hline
\hline
ImageTRM (ResNet) & Tiny Recursive & ResNet18 & 0.191 & 0.388 & Strong Baseline \\
\hline
ImageTRM (Swin) & Tiny Recursive & Swin-Tiny & 0.199 & \textbf{0.449} & \textbf{New SOTA} \\
\hline
ImageTRM (FuseLIP) & Tiny Recursive & FuseLIP & 0.155 & 0.378 & Competitive \\
\hline
\end{tabular}
\end{table*}

\textit{Description}: A table detailing the performance of the five main model variants, clearly differentiating between the backbones and the HRM configuration. The table includes columns for Model Variant, Backbone, H/M/L Config, Adaptive Computation Time Loss, ROUGE-L, and CIDEr.

\textbf{Analysis of Performance:}
The results in TABLE 1 confirm the synergistic relationship between deep hierarchical reasoning and advanced visual-language feature extraction.

\begin{enumerate} [label=\arabic*., nosep]
\item \textbf{HRM vs. Fixed-Depth Baselines:} The architectural capacity for deep latent reasoning is confirmed as a primary performance driver. The ImageHRM (Dual-Loop) model, despite using the foundational ResNet18 \cite{he2016resnet}, achieves significantly lower loss and higher validation accuracy compared to the fixed-depth ResNet+LSTM baseline, demonstrating that structured recurrence provides a crucial edge in algorithmic complexity.

\item \textbf{Dual vs. Triple Loop:} The transition to the Triple-Loop architecture (H-M-L) consistently provides measurable gains over the Dual-Loop configuration. This improvement validates the role of the intermediate Middle Loop ($z_M$), which is designed to enforce semantic clustering by anatomically or pathologically grouping findings \cite{reportstructure2020}, thereby optimizing the structural flow (Findings $\to$ Impression) as reflected by the increased ROUGE-L \cite{rouge2004} and CIDEr \cite{cider2015} scores.

\item \textbf{Impact of Advanced Backbones:} Successive integration of advanced visual encoders yields cumulative performance scaling. The \textbf{Swin Transformer} backbone \cite{liu2021swin}, chosen for its multi-scale feature extraction capabilities, further improves accuracy and CIDEr, underscoring the importance of high-quality visual grounding for pathological identification.

\item \textbf{The FuseLIP Advantage:} The ImageHRM (Triple) with FuseLIP \cite{schlarmann2025fuselip} achieves the strongest results across all metrics. The superior performance of this variant is attributed to the early fusion provided by the FuseLIP encoder. By receiving an intrinsically aligned multimodal embedding, the HRM core is relieved of the burden of basic modality alignment, allowing its deep recurrent cycles to focus entirely on the complex algorithmic task of structural and chronological planning for report generation.
\end{enumerate}

\subsection{Performance of Tiny Recursive Models (ImageTRM)}
Following the correction of the causal masking mechanism (Future Leakage bug), we re-evaluated the ImageTRM variants. Table \ref{tab:results_trm} presents the definitive results.
\begin{itemize}
    \item \textbf{Efficiency vs. Accuracy}: The ImageTRM-ResNet (7M parameters) achieves a remarkable CIDEr score of \textbf{0.388}, rivaling the much larger ImageHRM (Triple) baseline. This supports the "Less is More" hypothesis, suggesting that deep recursive reasoning ($N \times T$) can compensate for lower parameter counts.
    \item \textbf{The Swin Breakthrough}: Most notably, the \textbf{ImageTRM-Swin} variant achieves a new state-of-the-art CIDEr score of \textbf{0.449}, surpassing even the complex ImageHRM-FuseLIP system (0.438). This suggests that the hierarchical visual features of the Swin Transformer are uniquely suited to seed the "tiny" recursive core, allowing it to generate highly consensual clinical descriptions without the overhead of the Triple-Loop structure.
    \item \textbf{FuseLIP Instability}: While the FuseLIP variant performed well (CIDEr 0.378), it was outperformed by the simpler Swin integration in the tiny regime. This indicates that early-fusion backbones may require the larger capacity of the full HRM architecture to fully exploit their dense multimodal embeddings.
\end{itemize}

% \subsection{Qualitative Analysis}
% We visualize the captions generated by the model to understand the impact of hierarchical reasoning.

% \begin{figure}[h]
% \centering
% \fbox{\includegraphics[width=\columnwidth]{placeholder_sample_captions_and_reasoning_steps}}
% \caption{Sample Captions and Reasoning Steps}
% \end{figure}
% \textit{Description}: A grid of 4-5 images from ROCOv2 \cite{roco2020dataset}. Next to each image:
% \begin{enumerate} [label=\arabic*., nosep]
%\item \textbf{Ground Truth Caption}.
%\item \textbf{ImageHRM (Triple, FuseLIP) Prediction}.
%\item \textit{(Optional)}: Visualization of the M-state or H-state activity magnitude over sequential reasoning cycles, illustrating when the $z_M$ state updates to transition the semantic focus (e.g., transitioning from describing lung parenchyma findings to mediastinal findings).
% \end{enumerate}

% -------------------- CONCLUSION --------------------
\section{Conclusion}
We presented a comprehensive study on recursive reasoning for medical image captioning, introducing two distinct paradigms: the biologically-inspired \textbf{ImageHRM} and the efficiency-focused \textbf{ImageTRM}. Our Triple-Loop ImageHRM established that explicit hierarchical planning ($H \to M \to L$) significantly improves report structure compared to fixed-depth baselines. However, our most striking finding comes from the Tiny Recursive Model (TRM). By strictly applying the "Less is More" philosophy \cite{trm2025}, we demonstrated that a massive reduction in parameter count ($\sim$7M), when compensated by deep recursive execution and high-quality visual features (Swin Transformer), can achieve state-of-the-art performance (CIDEr 0.449). This result challenges the prevailing trend of ever-larger models, suggesting that for structured logic tasks, the \textit{depth of the reasoning chain} is more critical than the \textit{width of the network}. Future work will investigate hybrid architectures that combine the interpretability of HRM's structured loops with the efficient, full-gradient training of TRM.
\bibliographystyle{./IEEEtran}
\bibliography{ieee}
\end{document}